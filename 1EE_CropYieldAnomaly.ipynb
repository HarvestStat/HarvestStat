{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5270d915",
   "metadata": {},
   "source": [
    "# Calculate yield anomalies and plot them\n",
    "- Script to take the processed crop yield data and calculate crop yield anomalies. The script calculates three different types of anomalies to enable sensitivity analyses, and writes out new objects accordingly\n",
    "- In a second step these yield anomalies are written out to gridded netCDF objects for ease of use in other applications as this is the standard format for other gridded yield products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb34a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "import netCDF4\n",
    "import os, json\n",
    "import warnings\n",
    "import regionmask\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tools import save_hdf\n",
    "from tools import CreateLinkAdmin\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f113fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a moving average function\n",
    "def moving_average(a, smooth) :\n",
    "    n=smooth*2+1\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380f7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the crop yield data\n",
    "df = pd.read_hdf('./data/crop/adm_crop_production_ALL.hdf')\n",
    "#read in the FEWS admins\n",
    "shape = gpd.read_file('./data/shapefile/adm_current.shp').to_crs(\"EPSG:4326\")\n",
    "#merge the two\n",
    "df = pd.read_hdf('./data/crop/adm_crop_production_ALL.hdf')\n",
    "df = df.merge(shape[['FNID','ADMIN0','ADMIN1','ADMIN2']], left_on='fnid', right_on='FNID')\n",
    "df = df.rename(columns={'ADMIN1':'admin1','ADMIN2':'admin2','season_name':'season'})\n",
    "df = df[['fnid','country','admin1','admin2','product','season','harvest_month','harvest_year','indicator','value']]\n",
    "df['admin'] = df['fnid'].apply(lambda x: x[2:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5616c9",
   "metadata": {},
   "source": [
    "### Calculate anomalies and grid\n",
    "Below is the main portion of the script, where anomalies are calculated and written out as both a table and as a gridded netCDF product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03561753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fewer than 10 years for SO1990A21301, Maize, Gu\n",
      "fewer than 10 years for SO1990A22505, Maize, Gu\n",
      "fewer than 10 years for SO1990A22502, Maize, Gu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pk/_ym78shs7tx06w00zbcwxwgc0000gp/T/ipykernel_16529/675500990.py:104: UserWarning: No gridpoint belongs to any region. Returning an all-NaN mask.\n",
      "  msk = np.array(regionmask.Regions([fnGeos.loc[ifnx].geometry]).mask(lon,lat))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fewer than 10 years for KE2013A147, Maize, Long\n",
      "fewer than 10 years for ML2001A107, Maize, Main\n",
      "fewer than 10 years for TD2012A102, Maize, Main\n",
      "fewer than 10 years for NE2012A20204, Maize, Main season\n",
      "fewer than 10 years for NE2012A20205, Maize, Main season\n",
      "fewer than 10 years for NE2012A20210, Maize, Main season\n",
      "fewer than 10 years for NE2012A20407, Maize, Main season\n",
      "fewer than 10 years for NE2012A20516, Maize, Main season\n",
      "fewer than 10 years for NE2012A20519, Maize, Main season\n",
      "fewer than 10 years for NE2012A20603, Maize, Main season\n",
      "fewer than 10 years for NE2012A20607, Maize, Main season\n",
      "fewer than 10 years for NE2012A20611, Maize, Main season\n",
      "fewer than 10 years for NE2012A20614, Maize, Main season\n",
      "fewer than 10 years for NE2012A20616, Maize, Main season\n",
      "fewer than 10 years for NE2012A20710, Maize, Main season\n",
      "fewer than 10 years for NE2012A20711, Maize, Main season\n",
      "fewer than 10 years for NE2012A20712, Maize, Main season\n",
      "fewer than 10 years for NE2012A20715, Maize, Main season\n",
      "fewer than 10 years for NE2012A20805, Maize, Main season\n",
      "fewer than 10 years for NE2012A20806, Maize, Main season\n",
      "fewer than 10 years for KE2013A122, Wheat, Annual\n",
      "fewer than 10 years for KE2013A138, Wheat, Annual\n",
      "fewer than 10 years for ML2001A103, Wheat, Main\n"
     ]
    }
   ],
   "source": [
    "#use FNIDs and years to create an empty dataframe\n",
    "fnids = df.fnid.unique()\n",
    "yrs = np.sort(df.harvest_year.unique())\n",
    "dfEmpty = pd.DataFrame(columns = yrs, index=fnids,dtype=float)\n",
    "\n",
    "#get a table of FNIDs and geometries\n",
    "fnGeos = shape[['FNID','geometry']].drop_duplicates()\n",
    "fnGeos.set_index('FNID',inplace=True)\n",
    "\n",
    "#make an empty grid\n",
    "lon = np.arange(-20,55,0.1)\n",
    "lat = np.arange(40,-40,-0.1)\n",
    "\n",
    "cps = {\n",
    "    'maize':{'Somalia':['Maize','Gu'],\n",
    "            'Malawi':['Maize','Main'],\n",
    "            'Kenya':['Maize','Long'],\n",
    "            'Burkina Faso':['Maize','Main'],\n",
    "            'Mali':['Maize','Main'],\n",
    "            'Chad':['Maize','Main'],\n",
    "            'South Africa':['Maize (Yellow)','Summer'],\n",
    "            'Niger':['Maize','Main season'],\n",
    "            'Zambia':['Maize','Annual']},\n",
    "    'wheat':{'Kenya':['Wheat','Annual'],\n",
    "            'Mali':['Wheat','Main'],\n",
    "            'Chad':['Wheat','Main'],\n",
    "            'South Africa':['Wheat','Winter']}\n",
    "}\n",
    "\n",
    "dfYld = dfEmpty.copy()\n",
    "dfYldGau = dfEmpty.copy()\n",
    "dfYldGauAbs = dfEmpty.copy()\n",
    "dfYldSm5 = dfEmpty.copy()\n",
    "dfYldSm9 = dfEmpty.copy()\n",
    "\n",
    "for icr in ['maize','wheat']:\n",
    "    ncPath = '/Users/wanders7/Documents/Code/Project/NASA_GSCD/gscd/data/gridded/'+icr+'.nc'\n",
    "    ncf = netCDF4.Dataset(ncPath,'w',format='NETCDF4')\n",
    "    #create dimensions                            \n",
    "    ncf.createDimension('lon',lon.size)\n",
    "    ncf.createDimension('lat',lat.size)\n",
    "    ncf.createDimension('yr',yrs.size)\n",
    "    #create variables\n",
    "    ncf.createVariable('latitude','f4',('lat'))\n",
    "    ncf.createVariable('longitude','f4',('lon'))\n",
    "    ncf.createVariable('year','i8',('yr'))\n",
    "    #fill variables lat/lon\n",
    "    ncf.variables['latitude'][:]=lat\n",
    "    ncf.variables['longitude'][:]=lon    \n",
    "    ncf.variables['year'][:]=yrs\n",
    "    #create the yield variables and objects\n",
    "    ncf.createVariable('yield_pct','f4',('yr','lat','lon'))\n",
    "    ncf.createVariable('yield_abs','f4',('yr','lat','lon'))\n",
    "    \n",
    "    yld = np.ones([yrs.size,lat.size,lon.size])*np.nan\n",
    "    yldAbs = np.ones([yrs.size,lat.size,lon.size])*np.nan\n",
    "    \n",
    "    for icx in cps[icr].keys():\n",
    "        ipx = cps[icr][icx][0]\n",
    "        isx = cps[icr][icx][1]\n",
    "        ifnds = df.loc[(df['product']==ipx)&(df.country==icx)&(df.season==isx)].fnid.unique()\n",
    "        \n",
    "        for ifnx in ifnds:\n",
    "            shp = fnGeos.loc[ifnx].geometry\n",
    "            \n",
    "            ylds = df.loc[(df['product']==ipx)&(df.country==icx)&(df.season==isx)&\n",
    "                   (df.fnid==ifnx)&(df.indicator=='yield')].sort_values(by='harvest_year')\n",
    "            if ylds.value.size<10:\n",
    "                print('fewer than 10 years for '+ifnx+', '+ipx+', '+isx)\n",
    "                continue\n",
    "            \n",
    "            dfYld.loc[ifnx][ylds.harvest_year] = ylds.value\n",
    "            iYld = dfYld.loc[ifnx][ylds.harvest_year].values.astype(float)\n",
    "\n",
    "            #To account for a missing values in locations we can drop and re-weight the remaining values by counting non-zero values        \n",
    "            yldCount = np.copy(iYld)\n",
    "            yldCount[~np.isnan(yldCount)]=1\n",
    "            yldCount[np.isnan(yldCount)]=0\n",
    "            iYld[np.isnan(iYld)]=0\n",
    "            \n",
    "            #divide by the count to re-weight based on the # of values going into each smoothing filter\n",
    "            yldSm9Exp = moving_average(np.array(iYld),4)/moving_average(np.array(yldCount),4)\n",
    "            yldSm5Exp = moving_average(np.array(iYld),2)/moving_average(np.array(yldCount),2)\n",
    "            yldGauExp = ndimage.gaussian_filter1d(iYld,3)/ndimage.gaussian_filter1d(yldCount,3)\n",
    "            \n",
    "            stYldGauAbs = (iYld-yldGauExp)\n",
    "            #percent yield anom = (yld obs - yld exp)/yld exp\n",
    "            stYldGau = (iYld-yldGauExp)/yldGauExp\n",
    "            stYldSm9 = np.array(iYld[4:-4]-yldSm9Exp)/yldSm9Exp\n",
    "            stYldSm5 = np.array(iYld[2:-2]-yldSm5Exp)/yldSm5Exp\n",
    "            \n",
    "            #add nans back in\n",
    "            stYldGau[np.isnan(dfYld.loc[ifnx][ylds.harvest_year].values.astype(float))]=np.nan\n",
    "            stYldSm5[np.isnan(dfYld.loc[ifnx][ylds.harvest_year].values[2:-2].astype(float))]=np.nan\n",
    "            stYldSm9[np.isnan(dfYld.loc[ifnx][ylds.harvest_year].values[4:-4].astype(float))]=np.nan\n",
    "            stYldGauAbs[np.isnan(dfYld.loc[ifnx][ylds.harvest_year].values.astype(float))]=np.nan\n",
    "            \n",
    "            #write the empty data frames\n",
    "            dfYldGauAbs.loc[ifnx][ylds.harvest_year.values] = stYldGauAbs\n",
    "            dfYldGau.loc[ifnx][ylds.harvest_year.values] = stYldGau\n",
    "            dfYldSm5.loc[ifnx][ylds.harvest_year.values][2:-2] = stYldSm5\n",
    "            dfYldSm9.loc[ifnx][ylds.harvest_year.values][4:-4] = stYldSm9\n",
    "            #find the location mask\n",
    "            msk = np.array(regionmask.Regions([fnGeos.loc[ifnx].geometry]).mask(lon,lat))\n",
    "            #assign the yield time series to the relevant grid points based on the mask\n",
    "            yld.T[msk.T==0,...] = dfYldGau.loc[ifnx].values\n",
    "            yldAbs.T[msk.T==0,...] = dfYldGauAbs.loc[ifnx].values\n",
    "    #write the yield objects into the netCDF\n",
    "    ncf['yield_pct'][:] = yld\n",
    "    ncf['yield_abs'][:] = yldAbs\n",
    "    ncf.close();del ncf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41d39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23260330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ed5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02cdea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
